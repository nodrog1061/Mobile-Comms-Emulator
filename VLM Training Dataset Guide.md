# Screenshot Metadata Documentation
## Understanding the Metadata Files for VLM Training

Each ZIP file generated by the API now contains comprehensive metadata about every screenshot, perfect for VLM training and analysis.

---

## Files in Each ZIP:

```
whatsapp_screenshots_20241124_143022.zip
├── whatsapp_screenshot_1.jpg
├── whatsapp_screenshot_2.jpg
├── ... (all screenshot images)
├── metadata.json          ← Detailed JSON metadata
└── metadata.csv           ← Quick-view CSV format
```

---

## METADATA.JSON Structure

### Batch-Level Information:

```json
{
  "batch_info": {
    "platform": "whatsapp",
    "total_screenshots": 100,
    "generated_at": "2024-11-24T14:30:22.123456",
    "messages_before_image": 2,
    "messages_after_image": 2
  },
  "screenshots": [...]
}
```

### Per-Screenshot Information:

```json
{
  "screenshot_id": 1,
  "filename": "whatsapp_screenshot_1.jpg",
  "platform": "whatsapp",
  "contact_name": "Emma Wilson",
  "timestamp_generated": "2024-11-24T14:30:22.456789",
  
  "evidence_image": {
    "index": 3,
    "filename": "evidence_image_4",
    "position": "received"
  },
  
  "conversation": {
    "subreddit": "CasualConversation",
    "conversation_id": "7yy032",
    "title": "What's everyone up to this weekend?",
    "num_messages_before_image": 2,
    "num_messages_after_image": 2,
    "total_messages": 5
  },
  
  "messages": {
    "before_image": [
      {
        "sender": "received",
        "text": "Hey, what do you think about this?"
      },
      {
        "sender": "sent",
        "text": "Not sure, what is it?"
      }
    ],
    "after_image": [
      {
        "sender": "received",
        "text": "Pretty interesting right?"
      },
      {
        "sender": "sent",
        "text": "Yeah definitely"
      }
    ]
  }
}
```

---

## METADATA.CSV Structure

Simplified tabular format for quick analysis:

```csv
screenshot_id,filename,platform,subreddit,conversation_id,evidence_image_index,image_position,num_messages,contact_name
1,whatsapp_screenshot_1.jpg,whatsapp,CasualConversation,7yy032,3,received,5,Emma Wilson
2,whatsapp_screenshot_2.jpg,whatsapp,AskReddit,8ab123,1,sent,5,James Anderson
```

---

## Field Descriptions:

### Evidence Image Fields:

- **index**: Which evidence image (0-based index from your upload)
- **filename**: Reference name for the evidence image
- **position**: Where image appears ("sent" or "received" message)

### Conversation Fields:

- **subreddit**: Source subreddit from ConvoKit (or "generic" if generated)
- **conversation_id**: Original Reddit thread ID from ConvoKit
- **title**: Original Reddit post title
- **num_messages_before_image**: Count of messages before evidence
- **num_messages_after_image**: Count of messages after evidence
- **total_messages**: Total conversation items (including image)

### Messages Fields:

- **before_image**: Array of messages that appear before the evidence image
- **after_image**: Array of messages that appear after the evidence image
- Each message includes **sender** (sent/received) and **text** (cleaned content)

---

## Using Metadata for VLM Training

### Example 1: Label by Subreddit

```python
import json

with open('metadata.json') as f:
    data = json.load(f)

# Group screenshots by subreddit
by_subreddit = {}
for screenshot in data['screenshots']:
    subreddit = screenshot['conversation']['subreddit']
    if subreddit not in by_subreddit:
        by_subreddit[subreddit] = []
    by_subreddit[subreddit].append(screenshot['filename'])

print(f"CasualConversation: {len(by_subreddit['CasualConversation'])} screenshots")
```

### Example 2: Track Evidence Image Distribution

```python
# See which evidence images were used most
import pandas as pd

df = pd.read_csv('metadata.csv')
print(df['evidence_image_index'].value_counts())

# Result: Know if your VLM is biased toward certain images
```

### Example 3: Create Training Labels

```python
# Create labels for VLM training
with open('metadata.json') as f:
    data = json.load(f)

labels = []
for screenshot in data['screenshots']:
    labels.append({
        'image': screenshot['filename'],
        'contains_evidence': True,
        'evidence_position': screenshot['evidence_image']['position'],
        'context_subreddit': screenshot['conversation']['subreddit']
    })
```

### Example 4: Filter by Conversation Source

```python
import pandas as pd

df = pd.read_csv('metadata.csv')

# Get only AskReddit conversations
askreddit_screenshots = df[df['subreddit'] == 'AskReddit']
print(f"Found {len(askreddit_screenshots)} screenshots from AskReddit")

# Get only images in 'received' position
received_images = df[df['image_position'] == 'received']
```

---

## Subreddit Distribution Analysis

After generating, you can analyze the distribution:

```python
import json
from collections import Counter

with open('metadata.json') as f:
    data = json.load(f)

subreddits = [s['conversation']['subreddit'] for s in data['screenshots']]
distribution = Counter(subreddits)

print("Subreddit Distribution:")
for subreddit, count in distribution.most_common():
    percentage = (count / len(subreddits)) * 100
    print(f"  {subreddit}: {count} ({percentage:.1f}%)")
```

---

## VLM Training Use Cases

### 1. **Classification Training**
Use subreddit as a proxy for conversation topic/context:
- Label screenshots by subreddit type
- Train VLM to understand contextual differences
- Evaluate if context affects evidence detection

### 2. **Position Sensitivity Analysis**
Track if image position (sent vs received) affects detection:
```python
# Compare VLM accuracy on sent vs received images
sent_images = df[df['image_position'] == 'sent']
received_images = df[df['image_position'] == 'received']
```

### 3. **Conversation Complexity**
Use total_messages to study how conversation length affects detection:
```python
# Group by conversation complexity
simple = df[df['num_messages'] <= 4]
complex = df[df['num_messages'] > 4]
```

### 4. **Evidence Image Coverage**
Ensure all your evidence images are well-represented:
```python
# Check if any images are under/over-represented
df['evidence_image_index'].value_counts().plot(kind='bar')
```

---

## Example Analysis Script

```python
import json
import pandas as pd
from collections import Counter

# Load metadata
with open('metadata.json') as f:
    metadata = json.load(f)

# Convert to DataFrame
screenshots = metadata['screenshots']
df = pd.DataFrame([
    {
        'filename': s['filename'],
        'subreddit': s['conversation']['subreddit'],
        'evidence_idx': s['evidence_image']['index'],
        'position': s['evidence_image']['position'],
        'num_messages': s['conversation']['total_messages']
    }
    for s in screenshots
])

# Analysis
print("="*60)
print("SCREENSHOT BATCH ANALYSIS")
print("="*60)

print(f"\nTotal Screenshots: {len(df)}")
print(f"Unique Subreddits: {df['subreddit'].nunique()}")
print(f"Unique Evidence Images: {df['evidence_idx'].nunique()}")

print("\nSubreddit Distribution:")
print(df['subreddit'].value_counts())

print("\nEvidence Image Usage:")
print(df['evidence_idx'].value_counts())

print("\nImage Position:")
print(df['position'].value_counts())

print("\nMessage Count Distribution:")
print(df['num_messages'].value_counts().sort_index())
```

---

## Benefits for Your Dissertation

✅ **Reproducibility** - Know exact source of each training image  
✅ **Bias Detection** - Identify if certain subreddits dominate  
✅ **Stratified Sampling** - Ensure balanced test/train splits  
✅ **Error Analysis** - When VLM fails, check what subreddit/context  
✅ **Citation** - Can reference specific Reddit threads in your write-up  
✅ **Ethics Documentation** - Clear provenance of all training data  

---

## Quick Stats Commands

```bash
# Count screenshots by subreddit (from CSV)
cut -d',' -f4 metadata.csv | sort | uniq -c | sort -rn

# See all unique subreddits
cut -d',' -f4 metadata.csv | sort -u

# Count by image position
cut -d',' -f7 metadata.csv | sort | uniq -c
```

---

The metadata.json and metadata.csv files are automatically included in every ZIP you generate!
